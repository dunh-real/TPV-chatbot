# AI Chatbot RAG - Há»‡ thá»‘ng Há»i ÄÃ¡p TÃ i Liá»‡u Ná»™i Bá»™

## ğŸ“‹ Tá»•ng Quan

Há»‡ thá»‘ng chatbot sá»­ dá»¥ng RAG (Retrieval-Augmented Generation) Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u ná»™i bá»™ cÃ´ng ty. Há»‡ thá»‘ng sá»­ dá»¥ng:

- **OCR**: LightOnOCR-1B Ä‘á»ƒ trÃ­ch xuáº¥t text tá»« PDF
- **Chunking**: Small-to-Big Retrieval (2 táº§ng: Parent chunks theo Markdown Header, Child chunks theo Semantic)
- **Vector DB**: Qdrant Ä‘á»ƒ lÆ°u trá»¯ vÃ  tÃ¬m kiáº¿m embeddings
- **Embedding**: all-MiniLM-L6-v2
- **LLM**: Ollama (llama3.1 hoáº·c tÃ¹y chá»n)
- **Cache**: Redis Ä‘á»ƒ lÆ°u conversation history

## ğŸ—ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng

```
User Upload PDF â†’ OCR (LightOnOCR) â†’ Markdown
                                       â†“
                            Chunking 2 táº§ng:
                            1. Parent (theo Header 2)
                            2. Child (semantic)
                                       â†“
                            Embedding â†’ Qdrant
                                       
User Question â†’ Embedding â†’ Search Child Chunks (top-10)
                              â†“
                         Rerank (top-5)
                              â†“
                      Láº¥y Parent Chunks tÆ°Æ¡ng á»©ng
                              â†“
                    LLM (vá»›i context) â†’ JSON Response
                              â†“
                       Redis (lÆ°u history)
```

## ğŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c

```
ai-chatbot-rag/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ endpoints/
â”‚   â”‚       â”œâ”€â”€ upload.py       # Endpoint upload PDF
â”‚   â”‚       â”œâ”€â”€ chat.py         # Endpoint chat/ask
â”‚   â”‚       â””â”€â”€ health.py       # Health check
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py          # âœ… Configuration (HOÃ€N THÃ€NH)
â”‚   â”‚   â””â”€â”€ constants.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ocr_service.py     # ğŸ”„ Tiáº¿p theo: OCR processing
â”‚   â”‚   â”œâ”€â”€ chunking_service.py # ğŸ”„ Small-to-Big chunking
â”‚   â”‚   â”œâ”€â”€ vector_service.py  # ğŸ”„ Qdrant operations
â”‚   â”‚   â”œâ”€â”€ llm_service.py     # ğŸ”„ LLM interaction
â”‚   â”‚   â””â”€â”€ rag_service.py     # ğŸ”„ Orchestrator
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py         # âœ… Pydantic models (HOÃ€N THÃ€NH)
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py          # âœ… Logging utility (HOÃ€N THÃ€NH)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # PDF files
â”‚   â”œâ”€â”€ markdown/              # Markdown tá»« OCR
â”‚   â””â”€â”€ vector_store/          # Qdrant storage
â”œâ”€â”€ tests/
â”œâ”€â”€ .env                       # âœ… Environment variables (HOÃ€N THÃ€NH)
â”œâ”€â”€ .gitignore                 # âœ… Git ignore (HOÃ€N THÃ€NH)
â”œâ”€â”€ Dockerfile                 # ğŸ”„ Docker config
â”œâ”€â”€ docker-compose.yml         # ğŸ”„ Multi-service setup
â”œâ”€â”€ main.py                    # ğŸ”„ FastAPI app entry
â””â”€â”€ requirements.txt           # âœ… Dependencies (HOÃ€N THÃ€NH)
```

## ğŸš€ CÃ i Äáº·t

### 1. Clone vÃ  Setup Environment

```bash
# Clone repository (hoáº·c táº¡o project má»›i)
git clone https://github.com/dunh-real/TPV-chatbot
cd ai-chatbot-rag

# Táº¡o virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c: venv\Scripts\activate  # Windows

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### 2. Cáº¥u HÃ¬nh Environment Variables

Chá»‰nh sá»­a file `.env`:

```bash
# Qdrant
QDRANT_URL=http://localhost:6333

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379

# OCR Model path
OCR_MODEL_PATH=./models/LightOnOCR-1B

# LLM (Ollama)
LLM_BASE_URL=http://localhost:11434
LLM_MODEL_NAME=llama3.1
```

### 3. Khá»Ÿi Ä‘á»™ng Services

```bash
# Khá»Ÿi Ä‘á»™ng Qdrant (Docker)
docker run -p 6333:6333 -v $(pwd)/data/vector_store:/qdrant/storage qdrant/qdrant

# Khá»Ÿi Ä‘á»™ng Redis (Docker)
docker run -p 6379:6379 redis:7-alpine

# Khá»Ÿi Ä‘á»™ng Ollama (náº¿u chÆ°a cÃ³)
# Táº£i vá» tá»«: https://ollama.ai
ollama pull llama3.1
```

### 4. Download Models

```bash
# Táº£i LightOnOCR-1B
# (Theo hÆ°á»›ng dáº«n cá»§a LightOnOCR)
mkdir -p models
# Download model vÃ o models/LightOnOCR-1B/

# all-MiniLM-L6-v2 sáº½ tá»± Ä‘á»™ng download khi cháº¡y láº§n Ä‘áº§u
```

## ğŸ’» Sá»­ dá»¥ng

### Cháº¡y API Server

```bash
python main.py
# Hoáº·c: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### API Endpoints

#### 1. Upload PDF

```bash
curl -X POST "http://localhost:8000/api/v1/upload" \
  -F "file=@employee_handbook.pdf"
```

**Response:**
```json
{
  "success": true,
  "message": "File uploaded vÃ  xá»­ lÃ½ thÃ nh cÃ´ng",
  "document_id": "doc_abc123",
  "filename": "employee_handbook.pdf",
  "total_parent_chunks": 45,
  "total_child_chunks": 234,
  "processing_time_seconds": 12.5
}
```

#### 2. Chat/Ask Question

```bash
curl -X POST "http://localhost:8000/api/v1/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "ChÃ­nh sÃ¡ch nghá»‰ phÃ©p cá»§a cÃ´ng ty lÃ  gÃ¬?",
    "user_id": "user_123"
  }'
```

**Response:**
```json
{
  "question": "ChÃ­nh sÃ¡ch nghá»‰ phÃ©p cá»§a cÃ´ng ty lÃ  gÃ¬?",
  "answer": "Theo tÃ i liá»‡u ná»™i bá»™, nhÃ¢n viÃªn chÃ­nh thá»©c Ä‘Æ°á»£c hÆ°á»Ÿng 12 ngÃ y phÃ©p nÄƒm...",
  "sources": [
    {
      "chunk_id": "parent_chunk_123",
      "content": "## ChÃ­nh sÃ¡ch nghá»‰ phÃ©p...",
      "source_file": "employee_handbook_2024.pdf",
      "page_number": 15,
      "relevance_score": 0.95
    }
  ],
  "conversation_id": "conv_xyz789",
  "metadata": {
    "processing_time_seconds": 2.3,
    "model_used": "llama3.1"
  }
}
```

#### 3. Health Check

```bash
curl "http://localhost:8000/api/v1/health"
```

## ğŸ”§ Configuration

Táº¥t cáº£ cáº¥u hÃ¬nh náº±m trong file `.env`. CÃ¡c tham sá»‘ quan trá»ng:

| Tham sá»‘ | MÃ´ táº£ | Máº·c Ä‘á»‹nh |
|---------|-------|----------|
| `CHILD_CHUNK_SIZE` | KÃ­ch thÆ°á»›c child chunk | 512 |
| `TOP_K_CHILDREN` | Sá»‘ children ban Ä‘áº§u | 10 |
| `TOP_K_RERANK` | Sá»‘ chunks sau rerank | 5 |
| `LLM_TEMPERATURE` | Temperature cá»§a LLM | 0.1 |

## ğŸ§ª Testing

```bash
# Cháº¡y tests
pytest

# Vá»›i coverage
pytest --cov=app --cov-report=html
```

## ğŸ³ Docker

```bash
# Build image
docker build -t ai-chatbot-rag .

# Cháº¡y vá»›i docker-compose (bao gá»“m Qdrant, Redis)
docker-compose up -d
```

## ğŸ“ Roadmap

- [x] Setup cáº¥u trÃºc project
- [x] Config vÃ  schemas
- [x] OCR Service
- [x] Chunking Service  
- [x] Vector Service
- [x] LLM Service
- [x] RAG Service (orchestrator)
- [ ] API Endpoints
- [ ] Testing
- [ ] Docker deployment

## ğŸ¤ Contributing

1. Fork repository
2. Táº¡o branch má»›i (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Táº¡o Pull Request

## ğŸ“„ License

[MIT License](LICENSE)

## ğŸ‘¥ Team

TPV AI Engineering Team - Chatbot Project

---

**Note**: ÄÃ¢y lÃ  báº£n initial Ä‘á»ƒ format cáº¥u trÃºc thÆ° má»¥c chuáº©n chá»‰ cho cÃ¡c project vá» sau. Trong tÆ°Æ¡ng lai tÃ´i sáº½ (hoáº·c khÃ´ng) upload cÃ¡c file service (OCR, Chunking, Vector, LLM, RAG).