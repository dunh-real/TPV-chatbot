import sys
import os
import time

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from app.services.llm_service import OllamaChatLLM, RerankerService, PromptBuilder
from app.services.qdrant_service import VectorStoreService
from app.services.memory_service import RedisChatMemory

"""
Há»‡ thá»‘ng trÃ² chuyá»‡n:
- Input: Truy váº¥n Ä‘Æ°á»£c nháº­p tá»« ngÆ°á»i dÃ¹ng
- Output: Há»‡ thá»‘ng chatbot Ä‘Æ°a ra cÃ¢u tráº£ lá»i dá»±a trÃªn truy váº¥n tá»« nguá»“n tri thá»©c há»‡ thá»‘ng

Luá»“ng hoáº¡t Ä‘á»™ng:
    1. NgÆ°á»i dÃ¹ng nháº­p vÃ o truy váº¥n (Query Input). API tráº£ vá» cÃ¡c tham sá»‘ sau: tenant_id, access_role, employee_id
    2. ThÃªm ngá»¯ cáº£nh cho truy váº¥n dá»±a trÃªn lá»‹ch sá»­ há»™i thoáº¡i:
    Query Input + Conversation History -> LLM rewrite -> Context Query
    3. Embedding truy váº¥n:
    Context Query -> Embedding Model -> Dense Vector + Sparse Vector
    4. Truy váº¥n DB, tÃ¬m kiáº¿m ngá»¯ cáº£nh tÆ°Æ¡ng Ä‘á»“ng:
    Retrieval to Qdrant DB -> Context Docs (20)
    5. Xáº¿p háº¡ng ngá»¯ cáº£nh vÃ  láº¥y ra top ngá»¯ cáº£nh tá»‘i Æ°u:
    Context Docs (20) -> LLM reranking -> Context Docs (5)
    6. TÄƒng cÆ°á»ng context vÃ  Ä‘Æ°a vÃ o LLM Ä‘á»ƒ sinh ra pháº£n há»“i cuá»‘i cÃ¹ng:
    Context Query + Context Docs (5) -> LLM -> Final Response 
    
    Format Ä‘áº§u ra JSON:
        {       
        "tenant_id": tenant_id,
        "employee_id": employee_id,
        "query": query,
        "answer": final_answer,
        "citation": citation
        }
"""

# Load client service
db_client = VectorStoreService()
rerank_client = RerankerService()
prompt_client = PromptBuilder()
memory_client = RedisChatMemory()

class ChatSession():
    def __init__(self):
        pass

    def chat_session(self, tenant_id, access_role, employee_id):
        print("ğŸš€ KHá»I Äá»˜NG Há»† THá»NG RAG ENTERPRISE")
        print("="*50)
        print("ğŸ¤– CHáº¾ Äá»˜ TRÃ’ CHUYá»†N")
        print("="*50)

        # Khá»Ÿi táº¡o LLM
        try:
            llm_client = OllamaChatLLM()
            print(f"âœ… ÄÃ£ káº¿t ná»‘i model: {llm_client.model_name}")
        except Exception as e:
            print(f"âŒ Lá»—i khá»Ÿi táº¡o LLM: {e}")
            return
        
        # Loop chat
        while True:
            print("Nháº­p quit hoáº·c exit Ä‘á»ƒ káº¿t thÃºc cuá»™c trÃ² chuyá»‡n <3")
            query = input("\nğŸ‘¤ Báº¡n: ").strip()

            if query.lower() == 'quit' or query.lower() == 'exit':
                print("ğŸ‘‹ Táº¡m biá»‡t! Ráº¥t vui vÃ¬ Ä‘Æ°á»£c há»— trá»£")
                sys.exit(0)
            
            if not query: continue
            # Get conversation history 
            chat_history = memory_client.get_history(tenant_id, employee_id, limit=2)

            # Rewrite query input
            context_query = memory_client.contextualize_query(query, chat_history)

            memory_client.add_message(tenant_id, employee_id, "user", context_query)

            # first_time = time.time()

            try:
                print("   ğŸ” Äang tÃ¬m kiáº¿m thÃ´ng tin...")
                
                # Search vector
                search_results = db_client.search_hybrid(context_query, tenant_id, access_role, k=20)
                # Reranking docs
                top_docs = rerank_client.rerank(context_query, search_results, top_k=5)
                # Build prompt 
                messages = prompt_client.build_chat_messages(
                    query=context_query, 
                    search_results=top_docs, 
                    reasoning=False
                )

                print("   ğŸ§  AI Ä‘ang suy luáº­n...")
                
                response_obj, citation = llm_client.invoke(messages)
                
                final_answer = ""
                if hasattr(response_obj, 'content'):
                    final_answer = response_obj.content 
                else:
                    final_answer = str(response_obj) 

                # end_time = time.time() - first_time  

                # Save message to Redis
                memory_client.add_message(tenant_id, employee_id, "assistant", final_answer)

                # Output for Backend Team
                result = {
                    "tenant_id": tenant_id,
                    "employee_id": employee_id,
                    "query": query,
                    "answer": final_answer,
                    "citation": citation
                }
                
                # Print response
                print(f"\nğŸ¤– ChatBot: {final_answer}")
                print(result)
                # print(end_time)
                print("=" * 50)

            except Exception as e:
                print(f"âŒ Chi tiáº¿t lá»—i: {e}")

def main():
    chat_client = ChatSession()

    # API return: tenant_id, access_role, employee_id
    tenant_id = "VGP"
    access_role = 1
    employee_id = "B123"

    chat_client.chat_session(tenant_id, access_role, employee_id)

if __name__ == "__main__":
    main()