import sys
from pathlib import Path
from generation_service.llm_service import OllamaChatLLM
from generation_service.prompt_builder import PromptBuilder
from generation_service.reranker_service import RerankerService
from ingestion_service.converter import DocumentConverter
from ingestion_service.splitter import ChunkingService
from retrieval_service.qdrant_service import VectorStoreService

# Load client service
converter_client = DocumentConverter()
chunking_client = ChunkingService()
db_client = VectorStoreService()
rerank_client = RerankerService()
prompt_client = PromptBuilder()

# Gá»i API tráº£ vá» 2 tham sá»‘ sau:
tenant_id = "VGP"
role_user = "CEO"

def process_file_upload():
    """Quy trÃ¬nh náº¡p dá»¯ liá»‡u: Convert -> Split -> Embed -> Store"""
    print("\n" + "="*50)
    print("ğŸ“‚ CHáº¾ Äá»˜ UPLOAD TÃ€I LIá»†U")
    print("="*50)
    
    while True:
        file_path_str = input("ğŸ‘‰ Nháº­p Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i cá»§a file (hoáº·c 's' Ä‘á»ƒ skip): ").strip()
        
        if file_path_str.lower() == 's':
            return
        
        # Xá»­ lÃ½ Ä‘Æ°á»ng dáº«n
        file_path_str = file_path_str.replace('"', '').replace("'", "")
        path = Path(file_path_str)

        if not path.exists() or not path.is_file():
            print("âŒ File khÃ´ng tá»“n táº¡i hoáº·c Ä‘Æ°á»ng dáº«n sai. Vui lÃ²ng thá»­ láº¡i.")
            continue

        try:
            print("Äang Ä‘á»c file...")
            
            # BÆ¯á»šC 1: CONVERT
            markdown_text = converter_client.convert(path, save_debug=True)
            if not markdown_text:
                print("âŒ File rá»—ng hoáº·c khÃ´ng thá»ƒ convert.")
                continue

            # BÆ¯á»šC 2: CHUNKING (Hybrid Splitting)
            print("Äang chunking dá»¯ liá»‡u...")
            chunks = chunking_client.process_hybrid_splitting(
                text=markdown_text,
                tenant_id=tenant_id,
                filename=path.name,
                role_user=role_user
            )

            # BÆ¯á»šC 3: VECTOR STORE (Embed + Save)
            print("Äang thÃªm dá»¯ liá»‡u vÃ o Qdrant DB")
            db_client.add_chunks(chunks)
            
            # Tá»‘i Æ°u láº¡i index sau khi náº¡p
            db_client.optimize_indexing()
            
            print(f"ThÃªm dá»¯ liá»‡u thÃ nh cÃ´ng!")
            break 

        except Exception:
            print("Lá»—i xáº£y ra trong quÃ¡ trÃ¬nh xá»­ lÃ½ file :(")

def chat_session():
    """VÃ²ng láº·p trÃ² chuyá»‡n RAG"""
    print("\n" + "="*50)
    print("ğŸ¤– CHáº¾ Äá»˜ TRÃ’ CHUYá»†N")
    print("="*50)

    # Khá»Ÿi táº¡o LLM
    try:
        llm_client = OllamaChatLLM()
        print(f"âœ… ÄÃ£ káº¿t ná»‘i model: {llm_client.model_name}")
    except Exception as e:
        print(f"âŒ Lá»—i khá»Ÿi táº¡o LLM: {e}")
        return

    while True:
        print("Nháº­p quit hoáº·c exit Ä‘á»ƒ káº¿t thÃºc cuá»™c trÃ² chuyá»‡n <3")
        query = input("\nğŸ‘¤ Báº¡n: ").strip()

        if query.lower() == 'quit' or query.lower() == 'exit':
            print("ğŸ‘‹ Táº¡m biá»‡t! Ráº¥t vui vÃ¬ Ä‘Æ°á»£c há»— trá»£")
            sys.exit(0)
        elif query.lower() == 'u':
            return "UPLOAD_MODE"
        
        if not query: continue

        try:
            print("   ğŸ” Äang tÃ¬m kiáº¿m thÃ´ng tin...")
            
            # --- RAG PIPELINE ---
            # Search vector
            search_results = db_client.search_hybrid(query, tenant_id, role_user, k=20)
            # Reranking docs
            top_docs = rerank_client.rerank(query, search_results, top_k=5)
            
            messages = prompt_client.build_chat_messages(
                query=query, 
                search_results=top_docs, 
                reasoning=False
            )

            print("   ğŸ§  AI Ä‘ang suy luáº­n...")
            
            # Gá»ŒI HÃ€M INVOKE
            response_obj, citation = llm_client.invoke(messages)
            
            # Kiá»ƒm tra ká»¹ kiá»ƒu dá»¯ liá»‡u trÆ°á»›c khi in
            final_answer = ""
            if hasattr(response_obj, 'content'):
                final_answer = response_obj.content # Náº¿u lÃ  AIMessage
            else:
                final_answer = str(response_obj)    # Náº¿u lÃ  String hoáº·c Dict

            result = {
                "query": query,
                "answer": final_answer,
                "citation": citation
            }
            
            # In ra mÃ n hÃ¬nh
            print(f"\nğŸ¤– ChatBot: {final_answer}")
            print(result)
            print("-" * 30)

        except Exception as e:
            print(f"âŒ Chi tiáº¿t lá»—i: {e}")

def main():
    print("ğŸš€ KHá»I Äá»˜NG Há»† THá»NG RAG ENTERPRISE")
    
    # # Máº·c Ä‘á»‹nh vÃ o upload trÆ°á»›c
    # current_mode = "UPLOAD"
    
    # while True:
    #     if current_mode == "UPLOAD":
    #         process_file_upload()
    #         current_mode = "CHAT" # Upload xong tá»± Ä‘á»™ng chuyá»ƒn qua chat
        
    #     elif current_mode == "CHAT":
    #         signal = chat_session()
    #         if signal == "UPLOAD_MODE":
    #             current_mode = "UPLOAD"

    chat_session()

if __name__ == "__main__":
    main()